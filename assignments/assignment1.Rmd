---
title: 'Graphical and Markov Models: Assignment 1'
author: "Simon Schmetz"
date: "2025-04-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)

library(graph)
library(gRim)
```

# Tasks

1) Write a brief introduction to the data (where it comes from, what are the variables, etc.)

2) Try to fit different types of graphical log linear models to the data and select the most appropriate model.  Draw the graphs of the models you fit and comment on whether the independencies in the data make sense. 

3) Carry out a goodness of fit test to see whether the selected model fits the data.

4) Write some brief conclusions of your analysis.

# 1) Introduction

The dataset used in the following work on Graphical loglinear models is the "Student Alcohol Consumption" dataset (https://www.kaggle.com/datasets/uciml/student-alcohol-consumption?select=student-por.csv) with a wide array of variables and the rows corresponding to students behavior and background. From this dataset, the following variables are selected for the analysis from the Portuguese language course data:

sex - student's sex (binary: 'F' - female or 'M' - male)
traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
failures - number of past class failures (numeric: n if 1<=n<3, else 4)
schoolsup - extra educational support (binary: yes or no)
health - current health status (numeric: from 1 - very bad to 5 - very good)

These originally categorical variables with values 1:n are transformed in binary categorical with values corresponding to "high" and "low" with values <= 4 being low. A frequency column is then added and the dataset is used to create a contingency table. In these contingency tables we find many entries to be zero, corresponding to the dificulty to find a purely categorical dataset with even distribution across all variables. We however proceed with a lack of alternatives and keep this in mind when interpreting the results of models trained on this data.

```{r}
# Load the data
data <- read_csv("data/student-por.csv")



data <- data %>%
  rename_all(~ gsub(" ", "_", tolower(.)))

data = data[c("traveltime", "studytime", "failures", "sex", "schoolsup", "health")]

data <- data %>%
  mutate(
    sex = ifelse(sex == "F", 1, 0),         # 1 for Female, 0 for Male
    schoolsup = ifelse(schoolsup == "yes", 1, 0)
  )


# transform categorical variables
data <- data %>%
  mutate(across(all_of(c("traveltime","studytime","health","failures")), ~ case_when(
    . >= 3 ~ "high",
    . <= 2 ~ "low",
    TRUE ~ as.character(.)
  )))

# Add a frequency column and remove duplicate rows
data <- data %>%
  group_by(across(everything())) %>%
  mutate(Freq = n()) %>%
  ungroup()
data <- data %>%
  distinct(across(everything()), .keep_all = TRUE)

# Generate Contingency table
xtab_result <- xtabs(Freq ~ ., data = data)
print(xtab_result)
```

With the dataset set up, we begin with performing a chi square test of independence for each pair of variables. The resulting p-Values show some cases with the p-Values equal to zero or one, indicating there might be problems with performing the test, possibly due to the many zero frequencies as discussed above. We further observe some expected dependences like sex and school support, and some somewhat surprising indipendences like study time and failures for a significance level .05. 

```{r}
### Chi square test of indip

# get var pairs
vars <- names(data)[names(data) != "Freq"]
pairs <- combn(vars, 2, simplify = FALSE)

# perform Chi-square test for each pair
results <- lapply(pairs, function(pair) {
  xtab <- xtabs(Freq ~ ., data = data[, c(pair, "Freq")])
  test <- chisq.test(xtab)
  list(
    variables = pair,
    p_value = test$p.value,
    statistic = test$statistic,
    df = test$parameter,
    expected = test$expected
  )
})

# Print
for (res in results) {
  cat("\n---\n")
  cat("Variables:", paste(res$variables, collapse = " x "), " | p-value =", round(res$p_value, 3))
}

```



# 2) Fitting different types of graphical log linear models

With the initial set up of the data and the frequency table complete, we proceed with fitting different types of graphical log linear models to the data. We begin by defining a full model and a graphical model corresponding to the dependences of variables as shown by the chi-squared test with significance level .005.

```{r}
# Define full model
model_formular_full <- ~traveltime*studytime*failures*sex*schoolsup*health

# Define reduced model
model_formular_reduced  <- ~ traveltime*studytime+
  studytime*sex+
  studytime*schoolsup+
  sex*schoolsup+
  sex*health
  

```

We begin by visualizing the dependence structure in the full model vs the reduced model and checking if they both indeed are graphical models.

```{r}
par(mfrow=c(1,2))

# full model
m_full <- dmod(model_formular_full, data=xtab_result)
isGraphical(model_formular_full)
plot(m_full)
summary(m_full)
ug_full <- ug(model_formular_full)
rip(ug_full)

# reduced model
m_red <- dmod(model_formular_reduced, data=xtab_result)
isGraphical(model_formular_reduced) #
plot(m_red)
summary(m_red)
ug_red <- ug(model_formular_reduced)
rip(ug_red)
```

We then evaluate AIC and BIC for both models and find the reduced model to shot better (lower) AIC/BIC

```{r}
dmod(model_formular_full, data=xtab_result)
dmod(model_formular_reduced, data=xtab_result)
```

We then perform forward/backwards selection both restricted and unrestricted to find the optimal model. Overall, we find forward/backwards selection to yield the same results except for AIC-unrestricted, but different Results between AIC and BIC based selection. The results optained for restricted/unrestricted appear to be the same with the exception of backwards AIC unrestricted. Overall, travel time is shown to be indipendent  in all the resulting models, which is somewhat surprising as the chi-square test showed dependence between travel time and study time. Likewise, some models show the failure variable to be indipendent, also somewhat counter intuitive but in line with the chi-squared test results.

```{r}
# AIC - restricted 
par(mfrow=c(1,2))

m_full_selection = dmod(~.^.,data=xtab_result)
mbaic <- backward(m_full_selection)
plot(mbaic)

m_zero_selection = dmod(~.^1,data=xtab_result)
mfaic <- forward(m_zero_selection)
plot(mfaic)


# AIC - unrestricted 
par(mfrow=c(1,2))

m_full_selection = dmod(~.^.,data=xtab_result)
mbaic_unrestr <- backward(m_full_selection,type="unrestricted")
plot(mbaic_unrestr)


m_zero_selection = dmod(~.^1,data=xtab_result)
mfaic_unrestr <- forward(m_zero_selection,type="unrestricted")
plot(mfaic_unrestr)


# BIC - restricted 
par(mfrow=c(1,2))

m_full_selection = dmod(~.^.,data=xtab_result)
mbbic <- backward(m_full_selection,k=log(sum(xtab_result)))
plot(mbbic)


m_zero_selection = dmod(~.^1,data=xtab_result)
mfbic <- forward(m_zero_selection,k=log(sum(xtab_result)))
plot(mfbic)

# BIC - unrestricted 
par(mfrow=c(1,2))

m_full_selection = dmod(~.^.,data=xtab_result)
mbbic_unrestr <- backward(m_full_selection,type="unrestricted",k=log(sum(xtab_result)))
plot(mbbic_unrestr)


m_zero_selection = dmod(~.^1,data=xtab_result)
mfbic_unrestr <- forward(m_zero_selection,type="unrestricted",k=log(sum(xtab_result)))
plot(mfbic_unrestr)
```


# 3) Model Selection and Goodness of fit test

With the Models trained, the resulting models are compared based on AIC/BIC and the chi-squared test. The results show that the unrestricted models have a equal or better fit than the restricted models, while the differences are small. The AIC and BIC values are close to each other, indicating that the models are similar in terms of fit. The chi-squared test shows that all models selected via BIC fit the data well, with p-values greater than 0.05 while AIC selection based models barely not passing the threshhold. Overall, the key for the significant increase in goodness of fit for the BIC based models appears to be identifying the independence of the variable failures.

```{r}
# select bases on AIC/BIC

#
aic <- rep(NA,8)
aic[1] <- mbaic$fitinfo$aic
aic[2] <- mbaic_unrestr$fitinfo$aic
aic[3] <- mfaic$fitinfo$aic
aic[4] <- mfaic_unrestr$fitinfo$aic
aic[5] <- mbbic$fitinfo$aic
aic[6] <- mbbic_unrestr$fitinfo$aic
aic[7] <- mfbic$fitinfo$aic
aic[8] <- mfbic_unrestr$fitinfo$aic


bic <- rep(NA,8)
bic[1] <- mbaic$fitinfo$bic
bic[2] <- mbaic_unrestr$fitinfo$bic
bic[3] <- mfaic$fitinfo$bic
bic[4] <- mfaic_unrestr$fitinfo$bic
bic[5] <- mbbic$fitinfo$bic
bic[6] <- mbbic_unrestr$fitinfo$bic
bic[7] <- mfbic$fitinfo$bic
bic[8] <- mfbic_unrestr$fitinfo$bic


chisq_pval <- rep(NA,8)
chisq_pval[1] <- pchisq(mbaic$fitinfo$dev,mbaic$fitinfo$dimension[4])
chisq_pval[2] <- pchisq(mbaic_unrestr$fitinfo$dev,mbaic$fitinfo$dimension[4])
chisq_pval[3] <- pchisq(mfaic$fitinfo$dev,mfaic$fitinfo$dimension[4])
chisq_pval[4] <- pchisq(mfaic_unrestr$fitinfo$dev,mfaic$fitinfo$dimension[4])
chisq_pval[5] <- pchisq(mbbic$fitinfo$dev,mbbic$fitinfo$dimension[4])
chisq_pval[6] <- pchisq(mbbic_unrestr$fitinfo$dev,mbbic$fitinfo$dimension[4])
chisq_pval[7] <- pchisq(mfbic$fitinfo$dev,mfbic$fitinfo$dimension[4]) 
chisq_pval[8] <- pchisq(mfbic_unrestr$fitinfo$dev,mfbic$fitinfo$dimension[4])

selection_type = c("mbaic","mbaic_unrestr","mfaic","mfaic_unrestr","mbbic","mbbic_unrestr","mfbic","mfbic_unrestr")

model_seelction_results <- data.frame(
  Model = selection_type,
  AIC = aic,
  BIC = bic,
  Chi_Sq_pval = chisq_pval
)

model_seelction_results
```

Choosing a model with all BIC selection based models identical and the best AIC best model as mbaic_unrestr, but with the BIC models showing significantly better goodness of fit results while only marginal increase in AIC, the BIC models are heavily favoured in the choice of the model. 

```{r}
# Best models
best_aic_index <- which.min(model_seelction_results$AIC)
best_bic_index <- which.min(model_seelction_results$BIC)

best_aic <- model_seelction_results[best_aic_index, ]
best_bic <- model_seelction_results[best_bic_index, ]

# Print results
cat("====================================\n")
cat(" Best Model Based on AIC\n")
cat("====================================\n")
cat(sprintf("Model:           %s\n", best_aic$Model))
cat(sprintf("AIC:             %.2f\n", best_aic$AIC))
cat(sprintf("BIC:             %.2f\n", best_aic$BIC))
cat(sprintf("Chi-Sq p-value:  %.4f\n", best_aic$Chi_Sq_pval))

cat("\n====================================\n")
cat(" Best Model Based on BIC\n")
cat("====================================\n")
cat(sprintf("Model:           %s\n", best_bic$Model))
cat(sprintf("AIC:             %.2f\n", best_bic$AIC))
cat(sprintf("BIC:             %.2f\n", best_bic$BIC))
cat(sprintf("Chi-Sq p-value:  %.4f\n", best_bic$Chi_Sq_pval))

par(mfrow=c(1,2))
plot(mbaic_unrestr)
plot(mbbic)

```

# 4) Conclusions

For prediction tasks, the BIC based models are preferred as for good AIC, BIC and goodness-of-fit test results. However, when obtaining results from these models, the zero values in the initial frequency tables have to be taken into account. Bearing that in might, the models appear to represent the available data well.


